{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing a fully connected NN with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52525,
     "status": "ok",
     "timestamp": 1589959788926,
     "user": {
      "displayName": "Gennady Seryogin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBrZiOYSmshLKpQgkxC5Irxc8ZYqK62kr3Vwrh4g=s64",
      "userId": "05943810912303716558"
     },
     "user_tz": -180
    },
    "id": "8LBBQ6eAT2wf",
    "outputId": "0fb67687-8b58-48b4-846b-41f59025671b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive\n",
    "%cd 'My Drive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gltfofX8LJWc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcpe4wiTLJWh"
   },
   "outputs": [],
   "source": [
    "def count_mean_std(data_folder):\n",
    "    '''\n",
    "    Функция подсчитывает усредненные значения mean и std по всему датасету и возвращает их\n",
    "    '''\n",
    "    data = dset.ImageFolder(\n",
    "        root=data_folder,\n",
    "        transform = transforms.ToTensor())\n",
    "    \n",
    "    dataiter = iter(data)\n",
    "    \n",
    "    count = 0\n",
    "    for i in dataiter:\n",
    "        features, _ = i\n",
    "        if not count:\n",
    "            mean_sum = features.mean(axis=(2,3)).sum(axis=0)\n",
    "            std_sum = features.mean(axis=(2,3)).std(axis=0)\n",
    "        else:\n",
    "            mean_sum += features.mean(axis=(2,3)).sum(axis=0)\n",
    "            std_sum += features.mean(axis=(2,3)).std(axis=0)\n",
    "        count += features.shape[0]\n",
    "\n",
    "    mean_value = mean_sum / count\n",
    "    std_value = std_sum / count\n",
    "    \n",
    "    return (mean_value, std_value)\n",
    "\n",
    "def load_dataset(data_folder, batch_size):\n",
    "    '''\n",
    "    Функция подгружает данные из указанной директории и возвращает DataLoader объекты\n",
    "    '''\n",
    "    mean_value = [0.6007, 0.5609, 0.6516]\n",
    "    std_value = [0.0017, 0.0018, 0.0015]\n",
    "    \n",
    "    if not (mean_value or std_value):\n",
    "        mean_value, std_value = count_mean_std(data_folder)\n",
    "\n",
    "    data = dset.ImageFolder(\n",
    "        root=data_folder,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean_value, std=std_value)\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    test_split = 0.2\n",
    "    val_split = 0.2\n",
    "    test_split_ind = int(np.floor(test_split * len(data.imgs)))\n",
    "    val_split_ind = test_split_ind + int(np.floor(val_split * (len(data.imgs) - test_split_ind)))\n",
    "    indices = list(range(len(data.imgs)))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    test_indices, val_indices, train_indices = indices[:test_split_ind], indices[test_split_ind:val_split_ind], indices[val_split_ind:]                  \n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size = batch_size, sampler=train_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(data, batch_size = batch_size, sampler=val_sampler)\n",
    "    test_loader = torch.utils.data.DataLoader(data, batch_size = batch_size, sampler=test_sampler)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# initializing the data loaders\n",
    "batch_size = 64\n",
    "train_loader, val_loader, test_loader = load_dataset('./data/', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyv8SF9NLJWl"
   },
   "outputs": [],
   "source": [
    "class Flattener(nn.Module):\n",
    "    '''\n",
    "    вспомогательный модуль для превращения многомерного тензора в одномерный\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        batch_size, *_ = x.shape\n",
    "        return x.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VoMhPi9TLJWp"
   },
   "outputs": [],
   "source": [
    "im_resolution = 3*616*820\n",
    "\n",
    "nn_model = nn.Sequential(\n",
    "            Flattener(),\n",
    "            nn.Linear(im_resolution, 50),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(50, 5),\n",
    "         )\n",
    "nn_model.type(torch.FloatTensor)\n",
    "\n",
    "# We will minimize cross-entropy between the ground truth and\n",
    "# network predictions using Adam optimizer\n",
    "loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-3, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R8-u-ZgK8mdb"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs,\n",
    "                scheduler = StepLR(optimizer, step_size = 1, gamma = 1.0)):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        print('current epoch is ', epoch)\n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            print('current step is ', i_step)\n",
    "            prediction = model(x)\n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            indices = torch.argmax(prediction, 1) # 1 stands for the dimention to reduce (read more in the documentation)\n",
    "            correct_samples += torch.sum(indices == y) # using equation is suitable, because index and label are coincided\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(\"Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "    # TODO: Implement the inference of the model on all of the batches from loader,\n",
    "    #       and compute the overall accuracy.\n",
    "    # Hint: PyTorch has the argmax function!\n",
    "    correct_samples = 0\n",
    "    total_samples = 0\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        print('current val step is ', i)\n",
    "        prediction = model(x)\n",
    "        indices = torch.argmax(prediction, 1)\n",
    "        correct_samples += torch.sum(indices == y)\n",
    "        total_samples += y.shape[0]\n",
    "    \n",
    "    val_accuracy = float(correct_samples) / total_samples\n",
    "        \n",
    "    return val_accuracy\n",
    "\n",
    "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 3)\n",
    "loss_history_, train_history_, val_history_ = train_model(nn_model, train_loader, val_loader, loss, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1269206,
     "status": "ok",
     "timestamp": 1589969878622,
     "user": {
      "displayName": "Gennady Seryogin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBrZiOYSmshLKpQgkxC5Irxc8ZYqK62kr3Vwrh4g=s64",
      "userId": "05943810912303716558"
     },
     "user_tz": -180
    },
    "id": "TbsJd8g2ZSZz",
    "outputId": "2c8f51d4-343d-4046-a297-a842c9397678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current val step is  0\n",
      "current val step is  1\n",
      "current val step is  2\n",
      "current val step is  3\n",
      "current val step is  4\n",
      "current val step is  5\n",
      "current val step is  6\n",
      "current val step is  7\n",
      "current val step is  8\n",
      "current val step is  9\n",
      "current val step is  10\n",
      "current val step is  11\n",
      "current val step is  12\n",
      "current val step is  13\n",
      "current val step is  14\n",
      "current val step is  15\n",
      "current val step is  16\n",
      "current val step is  17\n",
      "current val step is  18\n",
      "current val step is  19\n",
      "current val step is  20\n",
      "current val step is  21\n",
      "current val step is  22\n",
      "current val step is  23\n",
      "Test accuracy:  0.23733333333333334\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = compute_accuracy(nn_model, test_loader)\n",
    "print('Test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_unlJWZg6AG"
   },
   "outputs": [],
   "source": [
    "loss = loss_history + loss_history_\n",
    "train_ac = train_history + train_history_\n",
    "val_ac = val_history + val_history_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1826,
     "status": "ok",
     "timestamp": 1589970468735,
     "user": {
      "displayName": "Gennady Seryogin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBrZiOYSmshLKpQgkxC5Irxc8ZYqK62kr3Vwrh4g=s64",
      "userId": "05943810912303716558"
     },
     "user_tz": -180
    },
    "id": "Sy6FfX5AmF_3",
    "outputId": "cfe31c3d-aa3a-4453-8545-4ff271c8d26a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0.23 Validation: 0.26\n",
      "Training: 0.27 Validation: 0.22\n",
      "Training: 0.3 Validation: 0.24\n",
      "Training: 0.33 Validation: 0.27\n",
      "Training: 0.35 Validation: 0.25\n",
      "Training: 0.35 Validation: 0.27\n",
      "Training: 0.38 Validation: 0.22\n",
      "Training: 0.43 Validation: 0.24\n"
     ]
    }
   ],
   "source": [
    "#zipped = list(zip(train_ac, val_ac))\n",
    "for i in zipped:\n",
    "    print('Training:', round(i[0], 2), 'Validation:', round(i[1], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KIsAxIFnBkk"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Despite the model is quite effective at finding new features and non-linear dependences, it doesn't perform a good result. The main reason is absence of sufficient number of hidden layers, which simplifies the way of classifying the images and leads to overfitting. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
